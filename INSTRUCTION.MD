# 🚀 AI 운세 서비스: 초보자를 위한 단계별 가이드

이 문서는 LLM(대규모 언어 모델)을 활용한 간단한 웹 애플리케이션을 처음부터 끝까지 직접 만들어보는 과정을 안내합니다. Python의 FastAPI 백엔드와 기본적인 HTML/CSS/JavaScript 프론트엔드를 사용하여, 사용자의 정보를 바탕으로 AI가 운세를 생성해주는 서비스를 구축합니다.

## 🎯 프로젝트 목표

- LLM 기반 웹 애플리케이션의 기본적인 구조 이해
- 로컬 환경에서 LLM 모델을 로드하고 실행하는 방법 학습
- 프롬프트 엔지니어링의 중요성과 실제 적용 방법 체험
- 프론트엔드와 백엔드 간의 데이터 통신 및 UI 업데이트 방법 학습
- 개발 과정에서 발생할 수 있는 일반적인 문제 해결 능력 향상

## 🛠️ 준비물

이 프로젝트를 시작하기 전에 다음 도구들이 설치되어 있는지 확인해주세요.

- **Python 3.8 이상**: [Python 공식 웹사이트](https://www.python.org/downloads/)에서 다운로드 및 설치
- **Git**: [Git 공식 웹사이트](https://git-scm.com/downloads)에서 다운로드 및 설치 (선택 사항이지만 권장)
- **웹 브라우저**: Chrome, Firefox 등 (개발자 도구 사용 권장)
- **터미널 또는 명령 프롬프트**: 코드 실행 및 서버 관리를 위해 필요

## 🏁 1단계: 프로젝트 설정

### 1.1 저장소 클론 (또는 파일 다운로드)

Git이 설치되어 있다면 다음 명령어로 프로젝트를 클론합니다.

```bash
git clone https://github.com/your-repo/local-ai-app.git # 실제 레포지토리 URL로 변경해주세요.
cd local-ai-app
```

Git이 없다면, 프로젝트 파일을 직접 다운로드하여 압축을 풀고 해당 디렉토리로 이동합니다.

### 1.2 Python 가상 환경 생성 및 활성화

프로젝트의 의존성을 격리하기 위해 가상 환경을 사용하는 것이 좋습니다.

```bash
python3 -m venv local-ai-app/ # 'local-ai-app/'은 가상 환경이 생성될 디렉토리입니다.
source local-ai-app/bin/activate # macOS/Linux
# local-ai-app\Scripts\activate # Windows (명령 프롬프트)
# local-ai-app\Scripts\Activate.ps1 # Windows (PowerShell)
```

가상 환경이 활성화되면 터미널 프롬프트 앞에 `(local-ai-app)`과 같은 표시가 나타납니다.

## 📦 2단계: 의존성 설치

활성화된 가상 환경에서 필요한 Python 라이브러리들을 설치합니다.

```bash
pip install fastapi uvicorn llama-cpp-python huggingface_hub
```

- `fastapi`: 백엔드 API를 구축하는 데 사용되는 웹 프레임워크
- `uvicorn`: FastAPI 애플리케이션을 실행하는 ASGI 서버
- `llama-cpp-python`: GGUF 형식의 LLM을 로컬에서 실행하기 위한 라이브러리
- `huggingface_hub`: Hugging Face 모델 허브에서 LLM 모델 파일을 다운로드하는 데 사용

## 🧩 3단계: 코드 이해하기

프로젝트의 주요 파일들을 간략히 살펴봅니다.

- `app/app.py` (또는 `app/app2.py`):
  - FastAPI 백엔드 애플리케이션의 핵심 코드입니다.
  - 사용자 요청을 받아 LLM을 호출하고, 운세 결과를 반환합니다.
  - LLM 모델 로드, 프롬프트 구성, 응답 처리 로직이 포함되어 있습니다.
  - `app2.py`는 `app.py`에서 `n_gpu_layers` 설정을 운영체제에 따라 동적으로 변경하는 예시입니다.
- `frontend/index.html`:
  - 웹 애플리케이션의 사용자 인터페이스(UI)를 정의하는 HTML 파일입니다.
  - 사용자 정보를 입력받는 폼과 운세 결과를 표시하는 영역이 있습니다.
- `frontend/style.css`:
  - `index.html`의 디자인과 레이아웃을 정의하는 CSS 파일입니다.
- `frontend/script.js`:
  - 프론트엔드의 동적인 동작을 처리하는 JavaScript 파일입니다.
  - 폼 제출 이벤트 처리, 백엔드 API 호출, 응답을 받아 UI에 표시하는 로직이 포함되어 있습니다.

## ▶️ 4단계: 애플리케이션 실행

프로젝트 루트 디렉토리(`local-ai-app/`)에서 다음 명령어를 실행하여 백엔드 서버를 시작합니다.

```bash
uvicorn app.app:app --host 0.0.0.0 --port 8000
# 또는 app2.py를 사용하려면:
# uvicorn app.app2:app --host 0.0.0.0 --port 8000
```

- **참고**: 이 명령어는 서버를 **포그라운드**에서 실행합니다. 터미널에 서버 로그가 실시간으로 출력되어 디버깅에 유용합니다. 서버를 종료하려면 `Ctrl + C`를 누르세요.
- **백그라운드 실행 (선택 사항)**: 개발이 완료된 후에는 `&`를 명령어 끝에 붙여 백그라운드에서 실행할 수 있습니다. `uvicorn app.app:app --host 0.0.0.0 --port 8000 &`

## 🌐 5단계: 웹 UI 사용하기

서버가 성공적으로 실행되면 웹 브라우저를 열고 다음 주소로 접속합니다.

```
http://0.0.0.0:8000
```

- 웹 페이지에 접속하여 이름, 생년월일, 태어난 시간, 성별, MBTI를 입력하고 "오늘의 운세 보기" 버튼을 클릭합니다.
- AI가 생성한 운세 결과가 화면에 표시되는 것을 확인합니다.

## 💡 초보자를 위한 핵심 학습 및 팁

이 프로젝트를 만들면서 겪었던 주요 어려움과 해결 과정을 통해 얻을 수 있는 중요한 교훈들입니다.

### 1. LLM 모델 선택과 런타임 (`llama-cpp-python`)

- **배경**: 초기에는 `transformers` 라이브러리로 작은 모델을 사용했지만, 로컬 환경에서 효율적인 실행을 위해 GGUF 형식의 모델로 전환했습니다.
- **학습**: GGUF 모델은 `llama-cpp-python`과 같은 경량 런타임에 최적화되어 있습니다. 이는 GPU가 없거나 제한적인 환경(예: M1/M2/M3 Mac의 Metal GPU)에서도 LLM을 로컬에서 실행할 수 있게 해줍니다.
- **팁**: `n_gpu_layers` 설정은 GPU 활용에 중요합니다. `-1`은 가능한 모든 레이어를 GPU로 오프로드하며, 메모리 부족 시 `0`으로 설정하거나 특정 레이어 수로 조절하여 CPU와 GPU 사용량의 균형을 맞출 수 있습니다.

### 2. 프롬프트 엔지니어링의 예술

- **배경**: LLM은 텍스트 생성에 뛰어나지만, 원하는 형식과 내용을 정확히 따르도록 하는 것은 매우 어렵습니다. 초기에는 JSON 형식 강제, 반복, 불필요한 내용 포함 등 다양한 문제가 있었습니다.
- **학습**:
  - **명확하고 간결한 지시**: 프롬프트는 모호함을 줄이고 모델이 수행해야 할 작업을 명확히 지시해야 합니다.
  - **부정적 지시 피하기**: "하지 마세요"보다는 "이렇게 하세요"와 같은 긍정적인 지시가 더 효과적입니다.
  - **역할 부여**: 모델에게 "전문가"와 같은 역할을 부여하면 해당 역할에 맞는 답변을 유도할 수 있습니다.
  - **반복 방지**: "같은 내용이나 비슷한 표현을 절대 반복하지 않습니다"와 같은 명시적인 지시가 필요합니다.
  - **형식 강제**: 특정 출력 형식을 요구할 때는 그 형식을 프롬프트에 직접 포함하고, 모델이 그 형식만 따르도록 강력하게 지시해야 합니다. (예: `--- 오늘의 운세 ---`와 같은 마커 사용)
- **팁**: 프롬프트는 한 번에 완성되지 않습니다. 지속적인 테스트와 반복적인 개선(Iterative Prompt Engineering)이 필수적입니다.

### 3. 리소스 관리와 최적화 (`n_ctx`, `max_tokens`)

- **배경**: LLM은 많은 메모리를 사용하며, `n_ctx` (컨텍스트 윈도우 크기)와 `max_tokens` (최대 생성 토큰 수) 설정은 시스템 리소스에 큰 영향을 미칩니다. 너무 큰 값은 메모리 부족으로 인한 서버 다운을 초래할 수 있습니다.
- **학습**: `n_ctx`는 프롬프트와 응답을 포함한 총 토큰 수이며, `max_tokens`는 응답의 최대 길이입니다. 이 두 값은 시스템의 RAM/VRAM 용량과 모델의 크기에 맞춰 신중하게 설정해야 합니다.
- **팁**: 메모리 문제가 발생하면 이 값들을 줄여보세요. 프롬프트가 길다면 `max_tokens`를 `n_ctx`보다 충분히 작게 설정하여 응답 공간을 확보해야 합니다.

### 4. 에러 핸들링 및 디버깅의 중요성

- **배경**: 웹 애플리케이션은 프론트엔드, 백엔드, LLM 등 여러 구성 요소가 상호작용하므로 다양한 유형의 오류가 발생할 수 있습니다.
- **학습**:
  - **서버 로그 확인**: 서버를 포그라운드에서 실행하여 실시간 로그를 확인하는 것이 문제 진단에 가장 효과적입니다. `JSONDecodeError`와 같은 백엔드 오류는 터미널에 출력됩니다.
  - **브라우저 개발자 도구**: 프론트엔드 오류(예: `404 Not Found`, `TypeError`)는 브라우저의 개발자 도구(Console, Network 탭)에서 확인할 수 있습니다.
  - **예외 처리**: `try-except` 블록을 사용하여 예상치 못한 오류(예: JSON 파싱 실패)를 처리하고 사용자에게 친화적인 메시지를 제공하는 것이 중요합니다.
- **팁**: 오류 메시지를 정확히 읽고, 어떤 계층(프론트엔드, 백엔드, LLM)에서 문제가 발생했는지 파악하는 것이 해결의 첫걸음입니다.

### 5. 사용자 경험(UX) 개선

- **배경**: 개발 초기에는 기능 구현에 집중하지만, 실제 사용자의 편의성도 중요합니다.
- **학습**: 입력 필드에 기본값을 제공하거나, 결과 표시 방식을 개선하는 등 작은 UI/UX 개선이 사용자 만족도를 크게 높일 수 있습니다.
- **팁**: 사용자의 입장에서 애플리케이션을 사용해보면서 불편한 점을 찾아 개선하는 습관을 들이세요.

## ❓ 문제 해결 (Troubleshooting)

- **`zsh: no such file or directory: local-ai-app/bin/uvicorn`**:
  - **원인**: 터미널의 현재 디렉토리가 프로젝트 루트 디렉토리(`local-ai-app/`)가 아니거나, 가상 환경이 활성화되지 않았을 수 있습니다.
  - **해결**: `cd local-ai-app` 명령어로 프로젝트 루트 디렉토리로 이동한 후, `source local-ai-app/bin/activate` (macOS/Linux) 또는 `local-ai-app\Scripts\activate` (Windows)로 가상 환경을 활성화합니다.
- **`Failed to load resource: the server responded with a status of 404 (Not Found)` (CSS/JS 파일)**:
  - **원인**: `index.html`에서 CSS나 JavaScript 파일의 경로가 잘못되었을 수 있습니다.
  - **해결**: `index.html` 파일에서 `<link rel="stylesheet" href="/static/style.css">`나 `<script src="/static/script.js"></script>`처럼 `/static/`을 붙여 수정해야 합니다.
- **`POST http://localhost:8000/fortune net::ERR_CONNECTION_REFUSED`**:
  - **원인**: 백엔드 서버(`uvicorn`)가 실행 중이 아닐 때 발생합니다.
  - **해결**: 터미널에서 `uvicorn app.app:app --host 0.0.0.0 --port 8000` 명령어를 실행하여 서버를 시작합니다.
- **`500 Internal Server Error` (서버 로그에 `JSON Decode Error` 또는 `AttributeError` 등)**:
  - **원인**: 백엔드 Python 코드에 오류가 있거나, LLM이 예상치 못한 형식의 응답을 반환했을 때 발생합니다.
  - **해결**: 서버를 포그라운드에서 실행하여 터미널에 출력되는 상세한 오류 메시지(Traceback)를 확인하고, 해당 라인의 코드를 수정합니다. LLM 응답 문제인 경우 프롬프트 수정이 필요할 수 있습니다.
- **운세 내용이 잘리거나 반복됨 (`* * * * * *` 또는 중복 문장)**:
  - **원인**: `max_tokens` 설정이 너무 작거나, 프롬프트가 모델에게 반복을 유도하는 방식으로 작성되었을 수 있습니다.
  - **해결**: `app.py`에서 `max_tokens` 값을 늘리거나, 프롬프트의 지시사항을 더 명확하고 간결하게 수정하여 모델이 반복하지 않도록 유도합니다. (예: "같은 내용이나 비슷한 표현을 절대 반복하지 않습니다.")
- **운세 내용이 원하는 형식과 다름**:
  - **원인**: LLM이 프롬프트의 지시사항을 정확히 따르지 못하고 있습니다.
  - **해결**: 프롬프트 엔지니어링 섹션의 팁을 참고하여 프롬프트를 지속적으로 개선해야 합니다. 모델에게 명확한 역할과 규칙을 부여하고, 원하는 출력 형식을 명시적으로 제시하는 것이 중요합니다.

---

이 가이드가 LLM 기반 애플리케이션 개발 여정에 도움이 되기를 바랍니다!
