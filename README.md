# AI 운세 서비스

## 🚀 프로젝트 개요

이 프로젝트는 사용자의 생년월일, 태어난 시간, 성별, MBTI 정보를 기반으로 오늘의 운세를 예측하고 조언을 제공하는 웹 애플리케이션입니다. FastAPI 백엔드와 간단한 HTML/CSS/JavaScript 프론트엔드로 구성되어 있으며, Hugging Face의 대규모 언어 모델(LLM)을 활용하여 운세 내용을 생성합니다.

## ✨ 주요 기능

- **사용자 정보 입력**: 이름, 생년월일, 태어난 시간, 성별, MBTI를 입력받습니다.
- **AI 기반 운세 생성**: 입력된 정보를 바탕으로 LLM이 맞춤형 운세를 생성합니다.
- **구조화된 운세 출력**: 운세 결과는 JSON 형식으로 백엔드에서 처리되어 프론트엔드에 구조화된 형태로 표시됩니다.
- **MBTI 맞춤 조언**: 사용자의 MBTI 특성을 반영한 구체적이고 현실적인 조언을 제공합니다.
- **간단한 웹 UI**: 사용자가 쉽게 정보를 입력하고 결과를 확인할 수 있는 웹 인터페이스를 제공합니다.

## 🛠️ 기술 스택

- **백엔드**: Python (FastAPI)
- **LLM**: Llama-3.2-3B-Instruct-Q4_K_M-GGUF (hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF)
- **LLM 런타임**: `llama-cpp-python`
- **모델 다운로드**: `huggingface_hub`
- **프론트엔드**: HTML, CSS, JavaScript

## 📦 설치 및 실행 방법

1.  **프로젝트 클론 (또는 파일 다운로드)**

    ```bash
    git clone <프로젝트_레포지토리_URL>
    cd local-ai-app
    ```

2.  **Python 가상 환경 활성화**

    프로젝트 루트 디렉토리에 있는 가상 환경을 활성화합니다.

    ```bash
    source local-ai-app/bin/activate
    ```

3.  **필요한 라이브러리 설치**

    ```bash
    pip install fastapi uvicorn llama-cpp-python huggingface_hub
    ```

4.  **애플리케이션 실행**

    ```bash
    uvicorn app.app:app --host 0.0.0.0 --port 8000
    ```

    (백그라운드 실행을 원하시면 `&`를 붙여주세요: `uvicorn app.app:app --host 0.0.0.0 --port 8000 &`)

5.  **웹 브라우저 접속**

    웹 브라우저를 열고 `http://0.0.0.0:8000` 에 접속합니다.

### 서버 재시작 및 종료

애플리케이션을 개발하거나 설정 변경 후에는 서버를 재시작해야 변경 사항이 적용됩니다.

- **서버 종료 (포그라운드 실행 시):**
    서버가 실행 중인 터미널에서 `Ctrl + C` 를 누르면 서버가 종료됩니다.

- **서버 종료 (백그라운드 실행 시):**
    1.  실행 중인 `uvicorn` 프로세스 ID를 찾습니다:
        ```bash
        pgrep -f uvicorn
        ```
    2.  찾은 프로세스 ID를 사용하여 프로세스를 종료합니다:
        ```bash
        kill <프로세스_ID>
        ```

- **서버 재시작:**
    서버를 종료한 후, 4번 항목의 `uvicorn` 실행 명령어를 다시 실행하면 됩니다.



## 💡 개발 과정 및 주요 고려사항

이 프로젝트는 LLM 기반 애플리케이션 개발의 여러 도전 과제를 해결하는 과정을 담고 있습니다.

### 1. 모델 선택 및 변경
- 초기에는 `microsoft/phi-2` 모델을 `transformers` 라이브러리로 사용했습니다.
- GGUF 형식의 모델(`Bllossom/llama-3.2-Korean-Bllossom-3B-gguf`, 이후 `hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF`)로 변경하면서 `llama-cpp-python` 라이브러리로 전환했습니다. 이는 GGUF 모델이 CPU 환경에 최적화되어 있기 때문입니다.

### 2. 프롬프트 엔지니어링
LLM이 원하는 형식과 내용으로 운세를 생성하도록 프롬프트를 지속적으로 개선했습니다.
- **초기 문제**: 모델이 무작위적인 내용, 불완전한 문장, 영어와 한국어가 섞인 출력, 불필요한 천문학적/미신적 내용을 생성하는 문제가 있었습니다.
- **개선 노력**: 
    - 모델의 역할을 명확히 정의하고, 출력 언어를 한국어로 엄격히 제한했습니다.
    - 운세 내용에 포함되어야 할 요소(MBTI 기반 조언, 연애운 포함)와 제외되어야 할 요소(사주, 별자리, 숫자 운세 등)를 명시했습니다.
    - **JSON 출력 형식 강제**: 가장 큰 도전 과제 중 하나였습니다. 모델이 특정 JSON 스키마를 따르도록 프롬프트에 JSON 구조를 직접 포함하고, 백엔드에서 LLM의 출력을 JSON으로 파싱하는 로직을 추가했습니다. 모델이 JSON을 완벽하게 생성하지 못할 경우를 대비하여 예외 처리도 구현했습니다.
    - 프롬프트의 순서와 간결성을 최적화하여 모델이 지시사항을 더 잘 이해하도록 했습니다.

### 3. 리소스 관리 (`n_ctx`, `max_tokens`)
- LLM 실행 시 `n_ctx` (컨텍스트 윈도우 크기)와 `max_tokens` (최대 생성 토큰 수) 설정이 중요했습니다.
- 초기에는 너무 큰 값으로 설정하여 MacBook M1 Pro (16GB RAM) 환경에서 메모리 부족으로 인한 서버 다운 문제가 발생했습니다.
- 최적의 성능과 안정성을 위해 이 값들을 2048로 조정했습니다. 이는 모델이 프롬프트를 충분히 이해하고 적절한 길이의 응답을 생성하면서도 시스템 리소스를 효율적으로 사용하도록 돕습니다.

### 4. 에러 핸들링 및 디버깅
- **프론트엔드-백엔드 통신 문제**: `404 Not Found` (정적 파일 경로 오류), `500 Internal Server Error` (백엔드 로직 오류), `ERR_CONNECTION_REFUSED` (서버 미실행) 등의 문제를 해결했습니다.
- **LLM 출력 파싱 오류**: LLM이 완벽한 JSON을 생성하지 못할 경우 `JSONDecodeError`가 발생할 수 있으므로, 이를 처리하고 원시 출력을 기록하여 디버깅에 활용하도록 했습니다.
- **실시간 로그 확인**: 서버를 포그라운드에서 실행하여 실시간 로그를 확인하는 것이 문제 진단에 매우 효과적이었습니다.

### 5. 사용자 경험 개선
- 초기에는 입력 필드를 매번 수동으로 채워야 하는 불편함이 있었습니다.
- `index.html`에 기본값을 미리 채워 넣어 테스트 및 사용 편의성을 높였습니다.
- 운세 결과가 생성되면 입력 폼을 숨기고 결과만 표시하도록 UI를 개선했습니다.

이 프로젝트는 LLM을 실제 애플리케이션에 통합하고 최적화하는 과정에서 겪을 수 있는 다양한 실질적인 문제들과 그 해결 방안을 보여줍니다.