# AI 운세 서비스

## 🚀 프로젝트 개요

이 프로젝트는 사용자의 생년월일, 태어난 시간, 성별, MBTI 정보를 기반으로 오늘의 운세를 예측하고 조언을 제공하는 웹 애플리케이션입니다. FastAPI 백엔드와 간단한 HTML/CSS/JavaScript 프론트엔드로 구성되어 있으며, Hugging Face의 대규모 언어 모델(LLM)을 활용하여 운세 내용을 생성합니다.

## ✨ 주요 기능

- **사용자 정보 입력**: 이름, 생년월일, 태어난 시간, 성별, MBTI를 입력받습니다.
- **AI 기반 운세 생성**: 입력된 정보를 바탕으로 LLM이 맞춤형 운세를 생성합니다.
- **구조화된 운세 출력**: 운세 결과는 JSON 형식으로 백엔드에서 처리되어 프론트엔드에 구조화된 형태로 표시됩니다.
- **MBTI 맞춤 조언**: 사용자의 MBTI 특성을 반영한 구체적이고 현실적인 조언을 제공합니다.
- **간단한 웹 UI**: 사용자가 쉽게 정보를 입력하고 결과를 확인할 수 있는 웹 인터페이스를 제공합니다.

## 🛠️ 기술 스택

- **백엔드**: Python (FastAPI)
    - 비동기 웹 애플리케이션 개발에 최적화된 고성능 웹 프레임워크로, LLM과의 비동기 통신 및 빠른 응답 처리에 적합하여 선택했습니다.
- **LLM**: Llama-3.2-3B-Instruct-Q4_K_M-GGUF (hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF)
    - 한국어 운세 생성에 적합한 성능을 가지면서도, 로컬 환경에서 실행 가능한 경량 모델을 탐색하던 중 GGUF 형식의 이 모델을 선정했습니다.
- **LLM 런타임**: `llama-cpp-python`
    - GGUF 형식의 LLM을 CPU 또는 Apple Silicon의 Metal GPU 환경에서 효율적으로 실행하기 위한 필수 라이브러리입니다. Python 바인딩을 제공하여 LLM 통합을 용이하게 합니다.
- **모델 다운로드**: `huggingface_hub`
    - Hugging Face 모델 허브에서 필요한 LLM 모델 파일을 손쉽게 다운로드하고 관리하기 위해 사용했습니다.
- **프론트엔드**: HTML, CSS, JavaScript
    - 빠르고 가볍게 사용자 인터페이스를 구축하고, 백엔드와 비동기적으로 통신하여 운세 결과를 동적으로 표시하기 위해 표준 웹 기술을 활용했습니다.

## 📦 설치 및 실행 방법

1.  **프로젝트 클론 (또는 파일 다운로드)**

    ```bash
    git clone <프로젝트_레포지토리_URL>
    cd local-ai-app
    ```

2.  **Python 가상 환경 활성화**

    프로젝트 루트 디렉토리에 있는 가상 환경을 활성화합니다.

    ```bash
    source local-ai-app/bin/activate
    ```

3.  **필요한 라이브러리 설치**

    ```bash
    pip install fastapi uvicorn llama-cpp-python huggingface_hub
    ```

4.  **애플리케이션 실행**

    ```bash
    uvicorn app.app:app --host 0.0.0.0 --port 8000
    ```

    (백그라운드 실행을 원하시면 `&`를 붙여주세요: `uvicorn app.app:app --host 0.0.0.0 --port 8000 &`)

5.  **웹 브라우저 접속**

    웹 브라우저를 열고 `http://0.0.0.0:8000` 에 접속합니다.

### 서버 재시작 및 종료

애플리케이션을 개발하거나 설정 변경 후에는 서버를 재시작해야 변경 사항이 적용됩니다.

- **서버 종료 (포그라운드 실행 시):**
    서버가 실행 중인 터미널에서 `Ctrl + C` 를 누르면 서버가 종료됩니다.

- **서버 종료 (백그라운드 실행 시):**
    1.  실행 중인 `uvicorn` 프로세스 ID를 찾습니다:
        ```bash
        pgrep -f uvicorn
        ```
    2.  찾은 프로세스 ID를 사용하여 프로세스를 종료합니다:
        ```bash
        kill <프로세스_ID>
        ```

- **서버 재시작:**
    서버를 종료한 후, 4번 항목의 `uvicorn` 실행 명령어를 다시 실행하면 됩니다.

## 💡 개발 과정 및 주요 고려사항

이 프로젝트는 LLM 기반 애플리케이션 개발의 여러 도전 과제를 해결하는 과정을 담고 있습니다.

### 1. 모델 선택 및 변경
- 초기에는 `microsoft/phi-2` 모델을 `transformers` 라이브러리로 사용했습니다.
- GGUF 형식의 모델(`Bllossom/llama-3.2-Korean-Bllossom-3B-gguf`, 이후 `hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF`)로 변경하면서 `llama-cpp-python` 라이브러리로 전환했습니다. 이는 GGUF 모델이 CPU 환경에 최적화되어 있기 때문입니다.

### 2. 프롬프트 엔지니어링
LLM이 원하는 형식과 내용으로 운세를 생성하도록 프롬프트를 지속적으로 개선했습니다.
- **초기 문제**: 모델이 무작위적인 내용, 불완전한 문장, 영어와 한국어가 섞인 출력, 불필요한 천문학적/미신적 내용을 생성하는 문제가 있었습니다.
- **개선 노력**: 
    - 모델의 역할을 명확히 정의하고, 출력 언어를 한국어로 엄격히 제한했습니다.
    - 운세 내용에 포함되어야 할 요소(MBTI 기반 조언, 연애운 포함)와 제외되어야 할 요소(사주, 별자리, 숫자 운세 등)를 명시했습니다.
    - **JSON 출력 형식 강제**: 가장 큰 도전 과제 중 하나였습니다. 모델이 특정 JSON 스키마를 따르도록 프롬프트에 JSON 구조를 직접 포함하고, 백엔드에서 LLM의 출력을 JSON으로 파싱하는 로직을 추가했습니다. 모델이 JSON을 완벽하게 생성하지 못할 경우를 대비하여 예외 처리도 구현했습니다.
    - 프롬프트의 순서와 간결성을 최적화하여 모델이 지시사항을 더 잘 이해하도록 했습니다.

### 3. 리소스 관리 (`n_ctx`, `max_tokens`, `n_gpu_layers`)
- LLM 실행 시 `n_ctx` (컨텍스트 윈도우 크기)와 `max_tokens` (최대 생성 토큰 수), 그리고 `n_gpu_layers` (GPU 오프로드 레이어 수) 설정이 중요했습니다.
- 초기에는 `n_ctx`와 `max_tokens`를 너무 큰 값으로 설정하여 MacBook M1 Pro (16GB RAM) 환경에서 메모리 부족으로 인한 서버 다운 문제가 발생했습니다.
- 최적의 성능과 안정성을 위해 `n_ctx`는 2048, `max_tokens`는 1500으로 조정했습니다. 이는 모델이 프롬프트를 충분히 이해하고 적절한 길이의 응답을 생성하면서도 시스템 리소스를 효율적으로 사용하도록 돕습니다.
- `n_gpu_layers`는 모델의 일부를 GPU로 오프로드하여 CPU 부담을 줄이고 속도를 향상시키는 역할을 합니다. 이 값은 시스템의 GPU 메모리 용량에 따라 최적화가 필요하며, `-1`은 가능한 모든 레이어를 GPU로 오프로드하라는 의미입니다. 만약 `n_gpu_layers=-1` 설정으로 인해 메모리 부족 또는 서버 다운 현상이 발생한다면, 시스템의 총 GPU 메모리 용량을 고려하여 이 값을 조정(예: 0 또는 특정 레이어 수)해야 합니다.

### 4. 에러 핸들링 및 디버깅
- **프론트엔드-백엔드 통신 문제**: `404 Not Found` (정적 파일 경로 오류), `500 Internal Server Error` (백엔드 로직 오류), `ERR_CONNECTION_REFUSED` (서버 미실행) 등의 문제를 해결했습니다.
- **LLM 출력 파싱 오류**: LLM이 완벽한 JSON을 생성하지 못할 경우 `JSONDecodeError`가 발생할 수 있으므로, 이를 처리하고 원시 출력을 기록하여 디버깅에 활용하도록 했습니다.
- **실시간 로그 확인**: 서버를 포그라운드에서 실행하여 실시간 로그를 확인하는 것이 문제 진단에 매우 효과적이었습니다.

### 5. 사용자 경험 개선
- 초기에는 입력 필드를 매번 수동으로 채워야 하는 불편함이 있었습니다.
- `index.html`에 기본값을 미리 채워 넣어 테스트 및 사용 편의성을 높였습니다.
- 운세 결과가 생성되면 입력 폼을 숨기고 결과만 표시하도록 UI를 개선했습니다.

### 6. LLM 상호작용 방식 변경 (create_completion -> create_chat_completion)
- `llama-cpp-python` 라이브러리에서 `llm.create_completion` 대신 `llm.create_chat_completion` 메서드를 사용하도록 변경했습니다. 이는 Llama 2/3 기반의 'Instruct' 또는 'Chat' 모델에 더 적합한 상호작용 방식입니다.
- 응답 추출 방식도 `output['choices'][0]['message']['content']`로 변경하여 chat 모델의 출력 구조에 맞췄습니다.

### 7. 프롬프트 내용 및 지시사항 최적화
- LLM이 JSON 형식만 출력해야 한다는 지시를 따르지 못하고 불필요한 텍스트를 생성하는 문제를 해결하기 위해 프롬프트 구조를 더욱 명확하게 개선했습니다.
- '이모티콘'이라는 모호한 단어를 'Emoji'로 변경하고, '사주, 별자리, 숫자 운세 등 불필요한 천문학적 또는 미신적인 내용은 절대 포함하지 마세요.'와 같은 부정적인 지시를 제거했습니다. 대신, '사주 운세처럼 흥미롭고 깊이 있는 통찰을 담아 운세를 작성해 주세요.'와 같이 긍정적인 방향으로 모델의 역할을 재정의했습니다.
- 프롬프트 내의 중복되거나 불필요한 문장들을 정리하여 모델이 핵심 지시사항에만 집중하도록 했습니다.

### 8. 프론트엔드 운세 결과 표시 개선
- LLM이 JSON 대신 문장 형식의 운세를 반환하도록 변경함에 따라, 프론트엔드(`script.js`)에서 LLM의 응답을 직접 받아 표시하도록 로직을 간소화했습니다.
- 사용자 입력 데이터(이름, 생년월일, 성별, MBTI 등)는 AI 응답에서 가져오는 대신, 프론트엔드 폼에서 직접 가져와 한국어 형식에 맞게 변환하여 표시하도록 수정했습니다. 이는 AI가 오직 운세 내용만 생성하도록 역할을 명확히 하고, 프론트엔드에서 데이터 표시의 일관성을 유지하기 위함입니다.

### 9. 프롬프트 구조 및 내용 최종 최적화
- LLM이 운세 내용과 행동 조언을 반복하거나 불필요한 정보를 포함하는 문제를 해결하기 위해 프롬프트 구조를 더욱 명확하게 개선했습니다.
- '운세 내용:'과 '행동 조언:' 같은 문구를 사용하지 않고, 운세와 행동 조언을 자연스럽게 이어지는 하나의 문단으로 작성하도록 지시했습니다.
- 각 섹션의 글자 수 제한(200자 이내)을 명확히 하고, 중복되거나 유사한 표현을 사용하지 않습니다.
- 긍정적이고 희망적인 분위기, Emoji 활용, 줄바꿈 없는 자연스러운 문단 흐름을 유지하도록 지시했습니다.

### 10. 프롬프트 간결화 및 명확화
- 프롬프트의 중복되는 문장과 내용을 제거하여 모델이 핵심 지시사항에만 집중하도록 최적화했습니다.

### 11. 프롬프트 최종 최적화 (반복 및 형식 준수 강화)
- LLM이 특정 문구를 반복하거나, 운세 내용과 행동 조언을 명확히 구분하지 못하는 문제를 해결하기 위해 프롬프트 구조를 더욱 강화했습니다.
- '운세 내용:'과 '행동 조언:'을 명확한 구분자로 사용하고, 모델이 이 형식에 맞춰 내용을 채우도록 지시했습니다.
- 각 섹션의 글자 수 제한을 다시 한번 강조하고, 반복을 피하도록 명시했습니다.

---

## ⚠️ Git 커밋 및 GitHub 푸시 과정에서 발생한 문제점 및 해결

이 프로젝트를 GitHub에 처음 푸시하는 과정에서 몇 가지 예상치 못한 문제에 직면했으며, 이를 해결하는 과정은 다음과 같습니다.

### 1. 대용량 파일로 인한 푸시 실패 (`Large files detected`)

- **문제점**: 프로젝트의 가상 환경(`local-ai-app/`) 내부에 포함된 `torch` 및 `tensorflow` 라이브러리의 `.dylib` 파일들이 GitHub의 단일 파일 크기 제한(100MB)을 초과하여 푸시가 거부되었습니다. (`libtorch_cpu.dylib` 184.26MB, `libtensorflow_cc.2.dylib` 548.76MB 등)
- **원인**: `.gitignore` 파일에 `local-ai-app/` 디렉토리를 무시하도록 설정했음에도 불구하고, Git 저장소 초기화 및 첫 커밋 과정에서 이 파일들이 Git의 인덱스에 포함되어 버렸습니다. Git은 `.gitignore`에 명시된 파일이라도 이미 추적되기 시작한 파일에 대해서는 무시 규칙을 적용하지 않습니다.
- **해결**:
    1.  **기존 `.git` 디렉토리 완전 삭제**: 로컬 프로젝트의 `.git` 디렉토리를 `rm -rf .git` 명령으로 완전히 삭제하여 기존의 모든 Git 기록(대용량 파일이 포함된 기록 포함)을 제거했습니다.
    2.  **새로운 Git 저장소 초기화**: `.gitignore` 파일이 이미 존재하는 상태에서 `git init`을 다시 실행하여 깨끗한 새 Git 저장소를 만들었습니다. 이로써 Git은 `.gitignore` 규칙을 처음부터 적용할 수 있게 되었습니다.

### 2. "커밋할 사항 없음" 반복 오류 및 API 토큰 초과 문제

- **문제점**: `git init` 후 `git add .`를 실행했음에도 불구하고 `git commit` 명령이 계속 "커밋할 사항 없음, 작업 폴더 깨끗함"이라고 보고하는 문제가 반복되었습니다. 또한, `git add .` 명령이 `.gitignore`에 의해 무시되는 `local-ai-app/` 디렉토리 내부의 파일들에 대해 대량의 경고 메시지를 출력하면서, 모델의 토큰 제한을 초과하는 API 오류를 유발했습니다.
- **원인**: Git의 내부 상태가 꼬였거나, `git add .`가 예상대로 작동하지 않아 변경사항을 제대로 감지하지 못하는 상황이 발생했습니다. 특히 대용량 경고 메시지 출력은 모델의 처리 한계를 초과하는 원인이 되었습니다.
- **해결**:
    1.  **`.git` 디렉토리 재삭제 및 재초기화**: Git 저장소의 상태가 불분명해질 때마다 `rm -rf .git` 후 `git init`을 반복하여 Git 저장소를 깨끗한 상태로 되돌렸습니다.
    2.  **필요한 파일 명시적 추가**: `git add .` 대신, `.gitignore`에 의해 무시되지 않아야 할 프로젝트의 핵심 파일들(예: `.gitignore`, `INSTRUCTION.MD`, `README-v1.md`, `README.md`, `app/`, `frontend/`, `screenshot.png`)만 **명시적으로 `git add <file1> <file2> ...` 명령에 지정하여 스테이징했습니다.** 이 방법으로 불필요한 파일들이 스테이징되지 않고, 대량의 경고 메시지 발생도 방지하여 토큰 초과 오류를 피할 수 있었습니다.

### 3. GitHub 원격 저장소 연결 및 푸시 성공

- **해결**: 위 문제점들을 해결한 후, 다음 단계를 통해 성공적으로 GitHub에 푸시했습니다.
    1.  **원격 저장소 URL 변경**: SSH 연결 문제(권한 오류)가 발생하여 `git remote set-url origin https://github.com/sangwon0707/local-ai-app.git` 명령을 사용하여 HTTPS URL로 변경했습니다.
    2.  **브랜치 이름 변경**: `git branch -M main` 명령으로 기본 브랜치 이름을 `main`으로 변경했습니다.
    3.  **최종 푸시**: `git push -u origin main` 명령을 통해 대용량 파일이 포함되지 않은 깨끗한 Git 저장소를 GitHub에 성공적으로 푸시했습니다.

이러한 과정을 통해 프로젝트의 Git 저장소를 올바르게 설정하고, GitHub의 제한 사항을 준수하며 코드를 성공적으로 공유할 수 있게 되었습니다.
