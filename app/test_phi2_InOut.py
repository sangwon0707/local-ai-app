from llama_cpp import Llama
from huggingface_hub import hf_hub_download

# 1. GGUF ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ìµœì´ˆ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œë¨)
model_name = "hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF"
model_file = "llama-3.2-3b-instruct-q4_k_m.gguf"
model_path = hf_hub_download(repo_id=model_name, filename=model_file)

# 2. ëª¨ë¸ ë¡œë“œ
llm = Llama(model_path=model_path, n_ctx=512, n_gpu_layers=-1, verbose=False)

while True:
    prompt = input("\nğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit'): ")
    if prompt.lower() == 'exit':
        print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    output = llm.create_completion(prompt, max_tokens=100, stream=False)
    result = output['choices'][0]['text']


    print("\nğŸ¤– ë‹µë³€:")
    print(result)
